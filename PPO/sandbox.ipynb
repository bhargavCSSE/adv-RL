{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fgsm_attack(observation, epsilon, data_grad):\n",
    "    observation = T.tensor(observation)\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_observation = observation + epsilon*sign_data_grad\n",
    "    return perturbed_observation.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ppo_torch import PPOagent\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('LunarLander-v2')    \n",
    "    load_checkpoint = True\n",
    "    render = False\n",
    "    n_trials = 10\n",
    "    n_games = 100\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    best_score = env.reward_range[0]\n",
    "    perturbation = 0.1\n",
    "\n",
    "    score_book = {}\n",
    "    actor_loss_book = {}\n",
    "    critic_loss_book = {}\n",
    "    total_loss_book = {}\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print('\\nTrial:', trial+1)\n",
    "        agent = PPOagent(n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha,\n",
    "                        n_epochs=n_epochs, input_dims=env.observation_space.shape,\n",
    "                        fc1_dims=256, fc2_dims=256, chkpt_dir='tmp/ppo')\n",
    "        \n",
    "        score_history = []\n",
    "        avg_score_history = []\n",
    "        loss = []\n",
    "        actor_loss = []\n",
    "        critic_loss = []\n",
    "        total_loss = []\n",
    "\n",
    "        learn_iters = 0\n",
    "        avg_score = 0\n",
    "        n_steps = 0\n",
    "        have_grad = False\n",
    "        data_grad = []\n",
    "\n",
    "        if load_checkpoint:\n",
    "            agent.load_models()\n",
    "\n",
    "        for i in tqdm(range(n_games)):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render(mode='human')\n",
    "                \n",
    "                action, prob, val = agent.choose_action(observation)\n",
    "                observation_, reward, done, info = env.step(action)\n",
    "\n",
    "                if (n_steps != 0) and (n_steps % N == 0):\n",
    "                    data_grad = advAgent.compute_grads()\n",
    "                    have_grad = True\n",
    "                if have_grad:\n",
    "                    observation_ = fgsm_attack(observation_, perturbation, data_grad)\n",
    "                \n",
    "                n_steps += 1\n",
    "                score += reward\n",
    "                agent.remember(observation, action, prob, val, reward, done)\n",
    "                advAgent.remember(observation, action, prob, val, reward, done)\n",
    "                \n",
    "                if not load_checkpoint:\n",
    "                    if n_steps % N == 0:\n",
    "                        loss.append(agent.learn())\n",
    "                        learn_iters += 1\n",
    "\n",
    "                observation = observation_\n",
    "            \n",
    "            if not load_checkpoint:\n",
    "                avg_loss = np.mean(loss, axis=0)\n",
    "                actor_loss.append(avg_loss[0])\n",
    "                critic_loss.append(avg_loss[1])\n",
    "                total_loss.append(avg_loss[2])\n",
    "\n",
    "            score_history.append(score)\n",
    "            avg_score = np.mean(score_history[-100:])\n",
    "            avg_score_history.append(avg_score)\n",
    "        \n",
    "        score_book[trial] = score_history\n",
    "        actor_loss_book[trial] = actor_loss\n",
    "        critic_loss_book[trial] = critic_loss\n",
    "        total_loss_book[trial] = total_loss\n",
    "\n",
    "            # print('episode', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score, 'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    print(\"\\nStoring rewards data...\")\n",
    "    a = pd.DataFrame(score_book)\n",
    "    a.to_csv('data/Blackbox/wSAC/PPO-LunarLander100x10-rewards-testFGSM_'+str(int(10*perturbation))+'.csv')\n",
    "    # if not load_checkpoint:\n",
    "    #     print(\"\\nStoring losses...\")\n",
    "    #     b = pd.DataFrame(actor_loss_book)\n",
    "    #     b.to_csv('data/PPO-LunarLander1000-actor_loss.csv')\n",
    "    #     c = pd.DataFrame(critic_loss_book)\n",
    "    #     c.to_csv('data/PPO-LunarLander1000-critic_loss.csv')\n",
    "    #     d = pd.DataFrame(total_loss_book)\n",
    "    #     d.to_csv('data/PPO-LunarLander1000-total_loss.csv')\n",
    "    print(\"Experiment finshed\")"
   ]
  }
 ]
}