{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitd78b596143a34f16905bf9c4066dc5d2",
   "display_name": "Python 3.8.5 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fgsm_attack(observation, epsilon, data_grad):\n",
    "    observation = T.tensor(observation)\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_observation = observation + epsilon*sign_data_grad\n",
    "    return perturbed_observation.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 1/1000 [00:00<02:39,  6.27it/s]\n",
      "Trial: 1\n",
      "  1%|          | 6/1000 [00:20<56:55,  3.44s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a922b2f48281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mdata_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                     \u001b[0mhave_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhave_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sandbox/PPO/ppo_torch.py\u001b[0m in \u001b[0;36mcompute_grads\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0mdata_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PyEnvs/py38/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PyEnvs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ppo_torch import Agent\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('LunarLander-v2')    \n",
    "    load_checkpoint = True\n",
    "    render = False\n",
    "    n_trials = 5\n",
    "    n_games = 1000\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    best_score = env.reward_range[0]\n",
    "\n",
    "    score_book = {}\n",
    "    actor_loss_book = {}\n",
    "    critic_loss_book = {}\n",
    "    total_loss_book = {}\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print('\\nTrial:', trial+1)\n",
    "        agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, alpha=alpha,\n",
    "                        n_epochs=n_epochs, input_dims=env.observation_space.shape)\n",
    "        \n",
    "        score_history = []\n",
    "        avg_score_history = []\n",
    "        loss = []\n",
    "        actor_loss = []\n",
    "        critic_loss = []\n",
    "        total_loss = []\n",
    "\n",
    "        learn_iters = 0\n",
    "        avg_score = 0\n",
    "        n_steps = 1\n",
    "        have_grad = False\n",
    "\n",
    "        if load_checkpoint:\n",
    "            agent.load_models()\n",
    "\n",
    "        for i in tqdm(range(n_games)):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render(mode='human')\n",
    "                \n",
    "                action, prob, val = agent.choose_action(observation)\n",
    "                observation_, reward, done, info = env.step(action)\n",
    "\n",
    "                if n_steps % N == 0:\n",
    "                    data_grad = agent.compute_grads()\n",
    "                    have_grad = True\n",
    "                if have_grad:\n",
    "                    observation_ = fgsm_attack(observation_, 0.9, data_grad)\n",
    "                    observation_ = np.mean(observation_, axis=0)\n",
    "                \n",
    "                n_steps += 1\n",
    "                score += reward\n",
    "                agent.remember(observation, action, prob, val, reward, done)\n",
    "                \n",
    "                if not load_checkpoint:\n",
    "                    if n_steps % N == 0:\n",
    "                        loss.append(agent.learn())\n",
    "                        learn_iters += 1\n",
    "\n",
    "                observation = observation_\n",
    "            \n",
    "            if not load_checkpoint:\n",
    "                avg_loss = np.mean(loss, axis=0)\n",
    "                actor_loss.append(avg_loss[0])\n",
    "                critic_loss.append(avg_loss[1])\n",
    "                total_loss.append(avg_loss[2])\n",
    "\n",
    "            score_history.append(score)\n",
    "            avg_score = np.mean(score_history[-100:])\n",
    "            avg_score_history.append(avg_score)\n",
    "        \n",
    "        score_book[trial] = score_history\n",
    "        actor_loss_book[trial] = actor_loss\n",
    "        critic_loss_book[trial] = critic_loss\n",
    "        total_loss_book[trial] = total_loss\n",
    "\n",
    "            # print('episode', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score, 'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    print(\"\\nStoring rewards data...\")\n",
    "    a = pd.DataFrame(score_book)\n",
    "    a.to_csv('data/PPO-LunarLander1000-rewards-testFGSM0_9.csv')\n",
    "    if not load_checkpoint:\n",
    "        print(\"\\nStoring losses...\")\n",
    "        b = pd.DataFrame(actor_loss_book)\n",
    "        b.to_csv('data/PPO-LunarLander1000-actor_loss.csv')\n",
    "        c = pd.DataFrame(critic_loss_book)\n",
    "        c.to_csv('data/PPO-LunarLander1000-critic_loss.csv')\n",
    "        d = pd.DataFrame(total_loss_book)\n",
    "        d.to_csv('data/PPO-LunarLander1000-total_loss.csv')\n",
    "    print(\"Experiment finshed\")"
   ]
  }
 ]
}